{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machine",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine Classifier\n",
        "* Supervised Learning Model\n",
        "* Mostly used for Classification\n",
        "* Used predominantly for Binary Classification\n",
        "* Hyperplane - similar to Decision boundary\n",
        "* Support Vectors - closes datapoints to each other from each class"
      ],
      "metadata": {
        "id": "tsQN5vhKiGDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HyperPlane\n",
        "Hyperplane is a line or a plane that separates the data points in 2 classes"
      ],
      "metadata": {
        "id": "75VX-xTilAYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vectors\n",
        "They are data points which lie nearest to the hyperplane. If these point changes, the position of the hyperplane changes."
      ],
      "metadata": {
        "id": "znCSrsdllOQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Margin\n",
        "Distance between support vectors of each class"
      ],
      "metadata": {
        "id": "UyUVbeoisXOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantages & Disadvantages\n",
        "1. Works well with smaller Dataset\n",
        "2. Works efficiently when there is a clear margin.\n",
        "3. Works well with more number of features.\n",
        "4. Not suitable with large dataset\n",
        "5. Not suitable with noisier dataset with overlapping dataset"
      ],
      "metadata": {
        "id": "25M6l2pYshYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation of the Hyperplane:\n",
        "\n",
        "y = wx - b"
      ],
      "metadata": {
        "id": "CVHWYbgFkBYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent:\n",
        "\n",
        "Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n",
        "\n",
        "w = w - α*dw\n",
        "\n",
        "b = b - α*db\n",
        "\n",
        "## Learning Rate:\n",
        "\n",
        "Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."
      ],
      "metadata": {
        "id": "fafTBGvykMFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing dependencies\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "l-dppEUHlCiV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine Classifier"
      ],
      "metadata": {
        "id": "oM2K1YG3lPZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import no_type_check\n",
        "class SVM_Classifier():\n",
        "  \n",
        "  # initiating the hyperparameters\n",
        "  def __init__(self, learning_rate, no_of_iterations, lambda_parameter):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "    self.lambda_parameter = lambda_parameter      \n",
        "  \n",
        "  # fitting dataset to SVM Classifier\n",
        "  def fit(self, x, y ):\n",
        "\n",
        "    # m is number of Data Points, number of rows\n",
        "    # n is number of Input features, number of columns\n",
        "    self.m, self.n = x.shape\n",
        "\n",
        "    # initiating weight and bias value\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "    # implementing gradient descent algorithm\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "\n",
        "  # updating weights and Bias\n",
        "  def update_weights(self):\n",
        "    \n",
        "    # label encoding - if else\n",
        "    y_label = np.where(self.y <= 0, -1, 1)\n",
        "\n",
        "    for index, x_i in enumerate(self.x):\n",
        "\n",
        "      condition = y_label[index] * np.dot((x_i, self.w) - self.b) >= 1\n",
        "\n",
        "      if condition == True:\n",
        "\n",
        "        dw = 2 * self.lambda_parameter * self.w\n",
        "        db = 0\n",
        "\n",
        "      else:\n",
        "        \n",
        "        dw = 2 * self.lambda_parameter * self.w - np.dot(x_i, y_label[index])\n",
        "        db = y_label[index]\n",
        "\n",
        "      self.w = self.w - self.learning_rate * dw\n",
        "\n",
        "      self.b = self.b - self.learning_rate * db\n",
        "  \n",
        "  # Predict the label for a given value\n",
        "  def predict(self, x):\n",
        "    \n",
        "    output = np.dot(x, self.w) - self.b\n",
        "\n",
        "    predicted_labels = np.sign(output)\n",
        "\n",
        "    y_hat = np.where(predicted_labels <= -1, 0, 1)\n",
        "\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "lhOqBGFIlM45"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "54lmBWWAWrSL"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}